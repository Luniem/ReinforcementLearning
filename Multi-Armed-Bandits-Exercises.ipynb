{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5226d22e",
      "metadata": {
        "id": "5226d22e"
      },
      "source": [
        "# Multi-Armed Bandits Exercises\n",
        "\n",
        "These exercises are will help you gain a deeper understanding for the mechanisms and behavior\\\n",
        "of $\\epsilon$-greedy action selection and sample-average based methods using k-armed bandit testbeds.\n",
        "\n",
        "**NOTICE:**\n",
        "1. You are allowed to work in groups of up to three people but **have to document** your group's\\\n",
        " members in the top cell of your notebook.\n",
        "2. **Comment your code**, explain what you do (refer to the slides). It will help you understand the topics\\\n",
        " and help me understand your thinking progress. Quality of comments will be graded.\n",
        "3. **Discuss** and analyze your results, **write-down your learnings**. These exercises are no programming\\\n",
        " exercises it is about learning and getting a touch for these methods. Such questions might be asked in the\\\n",
        " final exams.\n",
        " 4. Feel free to **experiment** with these methods. Change parameters think about improvements, write down\\\n",
        " what you learned. This is not only about collecting points for the final grade, it is about understanding\\\n",
        "  the methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "720f8015",
      "metadata": {
        "id": "720f8015"
      },
      "source": [
        "#### Provided Code - The Bandit class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6a9c53c9",
      "metadata": {
        "id": "6a9c53c9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Bandit:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_actions = 10\n",
        "        # Action values q*(a) are selected according to a normal distribution N(0,1)\n",
        "        #\n",
        "        self._action_values = np.random.randn(self.n_actions)\n",
        "\n",
        "    def action_space(self):\n",
        "        # For the sake of efficiency, the action space will always be from 0 to n_actions-1.\n",
        "        #\n",
        "        return np.arange(self.n_actions)\n",
        "\n",
        "    def take_action(self, action):\n",
        "        assert action in self.action_space(), \"invalid action selected, use action_space() to get a list of valid actions\"\n",
        "\n",
        "        # Actual rewards are selected according to a normal distribution N(mean(q*(At), 1)\n",
        "        # Compute reward for this action.\n",
        "        #\n",
        "        reward = self._action_values[action] + 1 * np.random.randn(1)\n",
        "        # Check if the optimal action was selected\n",
        "        #\n",
        "        is_optimal_action = np.isclose(self._action_values[action], self._action_values.max())\n",
        "        return reward, is_optimal_action\n",
        "\n",
        "    def action_values(self):\n",
        "        # Returns the real action values.\n",
        "        #\n",
        "        return self._action_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a1a1758",
      "metadata": {
        "id": "6a1a1758"
      },
      "source": [
        "### Exercise 1: Estimating Action-Values\n",
        "\n",
        "**Summary:**\n",
        "In this exercise you will use the sample-averaging method to estimate action-values for a k-armed bandit.\n",
        "\n",
        "**Provided Code:** Use the Bandit class from the cells above. Have a look at each method to figure out what it does.\n",
        "\n",
        "**Your Tasks in this exercise:**\n",
        "\n",
        "1. Implement the sample-averaging method.\n",
        "2. Use your implementation to estimate the action-values of the bandit.\n",
        "    * Compare your estimates with the real-values, how many iterations do you need?\n",
        "    * Discuss and document your results and learnings.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0dbb9805",
      "metadata": {
        "id": "0dbb9805"
      },
      "outputs": [],
      "source": [
        "# instantiate new bandit\n",
        "bandit = Bandit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print out action values\n",
        "print(bandit.action_values())\n",
        "print(bandit.action_values().max())"
      ],
      "metadata": {
        "id": "kwyj_SPA-7mz",
        "outputId": "e0109c58-de5c-4c77-c4ef-2a251e146695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kwyj_SPA-7mz",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.27271369 -0.1846686  -1.60040246 -0.50089394  0.21450077 -0.93501003\n",
            " -0.57733208 -0.53145666  0.45620653 -0.15294335]\n",
            "0.4562065339036664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a array where we store actions taken, rewards received and if it was optimal\n",
        "action_taken = []\n",
        "reward_received = []\n",
        "was_optimal_received = []\n",
        "possible_actions = bandit.action_space()\n",
        "\n",
        "max_steps = 10000\n",
        "curr_step = 0\n",
        "curr_action_pointer = 0\n",
        "\n",
        "# since we are not using the e-greedy action-selection for now just rotate through the actions to estimate them\n",
        "while curr_step < max_steps:\n",
        "  curr_action = possible_actions[curr_action_pointer]\n",
        "  curr_action_pointer = (curr_action_pointer + 1) % len(possible_actions)\n",
        "\n",
        "  reward, was_optimal = bandit.take_action(curr_action)\n",
        "\n",
        "  action_taken.append(curr_action)\n",
        "  reward_received.append(reward)\n",
        "  was_optimal_received.append(was_optimal)\n",
        "\n",
        "  curr_step += 1\n",
        "\n",
        "value_estimates = []\n",
        "\n",
        "# now we calculate the sample average of each action\n",
        "for action in possible_actions:\n",
        "  reward_sum = 0\n",
        "  action_n = 0\n",
        "\n",
        "  # go over all actions, rewards we did\n",
        "  for index, _ in enumerate(action_taken):\n",
        "    # if the taken action at the current step is the same as the one we currently look at -> sum it up and count it\n",
        "    if action_taken[index] == action:\n",
        "      reward_sum += reward_received[index]\n",
        "      action_n += 1\n",
        "\n",
        "  value_estimate = reward_sum / action_n\n",
        "  value_estimates.append(value_estimate)\n",
        "\n",
        "  print(value_estimate - bandit.action_values()[action])\n",
        ""
      ],
      "metadata": {
        "id": "TX2HYSOc_Ewh",
        "outputId": "bd55bbd0-d36f-4f1a-9485-40bfde7a9f36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TX2HYSOc_Ewh",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00497876]\n",
            "[0.00770929]\n",
            "[0.02152622]\n",
            "[-0.02601797]\n",
            "[-0.09742489]\n",
            "[0.00251112]\n",
            "[-0.02720286]\n",
            "[-0.02852138]\n",
            "[0.00913143]\n",
            "[0.03302234]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39fbd2d3",
      "metadata": {
        "id": "39fbd2d3"
      },
      "source": [
        "### Exercise 2 - $\\epsilon$-greedy Action-Selection\n",
        "\n",
        "**Summary:**\n",
        "In this exercise you will use $\\epsilon$-greedy action-selection with sample-averaging method to maximize the expected rewards in a k-armed bandit testbed.  \n",
        "\n",
        "**Provided Code:** Use the ```Bandit``` class from the cells above. Have a look at each method to figure out what it does.\n",
        "\n",
        "\n",
        "**Your Tasks in this exercise:**\n",
        "\n",
        "1. Implement the sample-averaging method using $\\epsilon$-greedy action-selection.\n",
        "2. Use your implementation to select the best actions using $\\epsilon$-greedy selection for 1000 time-steps.\n",
        "    * Use different values for $\\epsilon$, study the behavior.\n",
        "    * Plot the average rewards over 200 different bandits for each value of $\\epsilon$.\n",
        "    * Plot the percentage (over 200 different bandits) how often the optimal action was selected at each time-step for each value of $\\epsilon$.\n",
        "    * Compare your results with the plot in the slides.\n",
        "    * Discuss and document your results and learnings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5335ce5c",
      "metadata": {
        "id": "5335ce5c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7053a301",
      "metadata": {
        "id": "7053a301"
      },
      "source": [
        "### Exercise 3 - Nonstationary Bandit\n",
        "\n",
        "\n",
        "**Summary:**\n",
        "In this exercise you will implement a nonstationary k-armed bandit testbed and use the sample-averaging\\\n",
        "method on it.\n",
        "\n",
        "**Provided Code:** None\n",
        "\n",
        "\n",
        "**Your Tasks in this exercise:**\n",
        "\n",
        "1. Extend the ```Bandit``` class such that the reward distributions are nonstationary. To do so:\n",
        "    * Initialize all $q_*(a)$ to the same value.\n",
        "    * On each step, take a step in an independent random walk for each action (you can do this by\\\n",
        "      adding a normally distributed value with mean 0 and standard devition of 0.01 to all $q_*(a)$\\\n",
        "      on each step).\n",
        "2. Run the sample-averaging method with $\\epsilon$-greedy action-selection using the nonstationary bandit.\n",
        "    * Reproduce the plots from exercise 2 using the nonstationary bandit.\n",
        "    * Interpret, discuss (and document) your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b43bd03",
      "metadata": {
        "id": "5b43bd03"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "56553f8b",
      "metadata": {
        "id": "56553f8b"
      },
      "source": [
        "### Exercise 4 - Average-Sampling with Constant Step-Size\n",
        "\n",
        "\n",
        "**Summary:**\n",
        "In this exercise you will use constant step-size sample-averaging to maximize the\\\n",
        "reward in a nonstationary k-armed bandit testbed.\n",
        "\n",
        "**Provided Code:** None\n",
        "\n",
        "\n",
        "**Your Tasks in this exercise:**\n",
        "\n",
        "1. Implement the sample-averaging method using $\\epsilon$-greedy action-selection with a constant step-size and incremental updates.\n",
        "2. Use your implementation of the nonstationary bandit and your sample-averaging method with constant-step size\\\n",
        "   to select the best actions.\n",
        "   * Use  $\\epsilon = 0.1, \\alpha=0.1$\n",
        "   * Use more time-steps such as $10000$\n",
        "   * Plot, discuss (and document) your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaf8e6a",
      "metadata": {
        "id": "adaf8e6a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}