{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nico KnÃ¼nz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaS5EMq4Ugla"
      },
      "source": [
        "# Policy Gradient Exercises\n",
        "\n",
        "**NOTICE:**\n",
        "1. You are allowed to work in groups of up to three people but **have to document** your group's\\\n",
        " members in the top cell of your notebook.\n",
        "2. **Comment your code**, explain what you do (refer to the slides). It will help you understand the topics\\\n",
        " and help me understand your thinking progress. Quality of comments will be graded.\n",
        "3. **Discuss** and analyze your results, **write-down your learnings**. These exercises are no programming\\\n",
        " exercises it is about learning and getting a touch for these methods. Such questions might be asked in the\\\n",
        " final exams.\n",
        " 4. Feel free to **experiment** with these methods. Change parameters think about improvements, write down\\\n",
        " what you learned. This is not only about collecting points for the final grade, it is about understanding\\\n",
        "  the methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "7b5nvhcqUglb",
        "outputId": "3ab49910-d09f-46a4-f60f-01f060f15192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in ./rl/lib/python3.12/site-packages (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in ./rl/lib/python3.12/site-packages (from gymnasium) (2.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in ./rl/lib/python3.12/site-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in ./rl/lib/python3.12/site-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in ./rl/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: mujoco==2.3.0 in ./rl/lib/python3.12/site-packages (2.3.0)\n",
            "Requirement already satisfied: absl-py in ./rl/lib/python3.12/site-packages (from mujoco==2.3.0) (2.4.0)\n",
            "Requirement already satisfied: glfw in ./rl/lib/python3.12/site-packages (from mujoco==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: numpy in ./rl/lib/python3.12/site-packages (from mujoco==2.3.0) (2.4.2)\n",
            "Requirement already satisfied: pyopengl in ./rl/lib/python3.12/site-packages (from mujoco==2.3.0) (3.1.10)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"MUJOCO_PATH\"] = \"/home/nico/.mujoco/mujoco-2.3.0\"\n",
        "os.environ[\"MUJOCO_PLUGIN_PATH\"] = \"/home/nico/.mujoco/mujoco-2.3.0/plugin\"\n",
        "\n",
        "# If you run on google-colab you have to install this package whenever you start a kernel\n",
        "#\n",
        "!pip install gymnasium\n",
        "!pip install mujoco==2.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKDbWGKjUglb"
      },
      "source": [
        "### Exercise 1 - REINFORCE\n",
        "\n",
        "**Summary:** Implement the REINFORCE algorithm and use it to solve the ```CartPole-v1``` environment.\n",
        "\n",
        "\n",
        "**Provided Code:** Feel free to re-use code from previous exercises.\n",
        "\n",
        "\n",
        "**Your Tasks in this exercise:**\n",
        "1. Implement REINFORCE\n",
        "2. Solve the ```CartPole-v1``` environment.\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Bjo5Dqi7Uglc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-03 00:05:45.860303: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1770073547.304691   19780 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3721 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:07:00.0, compute capability: 7.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0, Avg Reward: 108.5\n",
            "Episode 25, Avg Reward: 123.0\n",
            "Episode 50, Avg Reward: 57.6\n",
            "Episode 75, Avg Reward: 57.1\n",
            "Episode 100, Avg Reward: 51.1\n",
            "Episode 125, Avg Reward: 41.6\n",
            "Episode 150, Avg Reward: 43.2\n",
            "Episode 175, Avg Reward: 49.7\n",
            "Episode 200, Avg Reward: 72.7\n",
            "Episode 225, Avg Reward: 60.1\n",
            "Episode 250, Avg Reward: 61.1\n",
            "Episode 275, Avg Reward: 53.4\n",
            "Episode 300, Avg Reward: 55.7\n",
            "Episode 325, Avg Reward: 58.7\n",
            "Episode 350, Avg Reward: 50.9\n",
            "Episode 375, Avg Reward: 58.1\n",
            "Episode 400, Avg Reward: 58.3\n",
            "Episode 425, Avg Reward: 95.4\n",
            "Episode 450, Avg Reward: 132.5\n",
            "Episode 475, Avg Reward: 328.3\n",
            "Episode 500, Avg Reward: 270.2\n",
            "Episode 525, Avg Reward: 179.7\n",
            "Episode 550, Avg Reward: 407.8\n",
            "Episode 575, Avg Reward: 500.0\n",
            "==== Solved! ====\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "gamma = 0.99\n",
        "alpha = 1e-3\n",
        "\n",
        "# create a policy network\n",
        "def create_policy_net(state_dim, action_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(state_dim,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(action_dim, activation='softmax')  # output probabilities for action\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# instantiate the policy network\n",
        "policy_net = create_policy_net(4, 2)\n",
        "optimizer = tf.keras.optimizers.Adam(alpha)\n",
        "\n",
        "# function to generate an episode using the current policy\n",
        "def generate_episode(policy_net):\n",
        "    # create lists to store states, actions, rewards\n",
        "    states, actions, rewards = [], [], []\n",
        "\n",
        "    # reset the environment\n",
        "    s, _ = env.reset()\n",
        "    # loop until the episode is done\n",
        "    while True:\n",
        "        # get action probabilities from the policy network\n",
        "        s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
        "        # sample an action according to the porbabilities\n",
        "        probs = policy_net(s_tensor)[0].numpy()\n",
        "        a = np.random.choice(len(probs), p=probs)\n",
        "\n",
        "        # take the action in the environment\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        # store the transition\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        # check if episode is done\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "        # move to the next state\n",
        "        s = s_next\n",
        "\n",
        "    # return the lists\n",
        "    return states, actions, rewards\n",
        "\n",
        "# function to compute returns\n",
        "def compute_returns(rewards, gamma):\n",
        "    # start with an array of zeros\n",
        "    G = np.zeros(len(rewards))\n",
        "\n",
        "    # compute the returns backwards and store them in G\n",
        "    running_sum = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        running_sum = rewards[t] + gamma * running_sum\n",
        "        G[t] = running_sum\n",
        "    return G\n",
        "\n",
        "# functiono to update the policy network\n",
        "def reinforce_update(policy_net, states, actions, returns):\n",
        "    # convert to tensors\n",
        "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "    actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "    returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # compute the loss\n",
        "        probs = policy_net(states)\n",
        "        action_masks = tf.one_hot(actions, depth=2)\n",
        "        selected_probs = tf.reduce_sum(probs * action_masks, axis=1)\n",
        "\n",
        "        # compute log probabilities\n",
        "        log_probs = tf.math.log(selected_probs + 1e-8)\n",
        "\n",
        "        # compute the loss as negative expected return\n",
        "        loss = -tf.reduce_mean(log_probs * returns)  \n",
        "\n",
        "    # compute gradients and apply them\n",
        "    grads = tape.gradient(loss, policy_net.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, policy_net.trainable_variables))\n",
        "    \n",
        "solved = False\n",
        "\n",
        "# training loop\n",
        "for episode in range(2000):\n",
        "    # generate an episode\n",
        "    states, actions, rewards = generate_episode(policy_net)\n",
        "    # compute returns for the episode\n",
        "    returns = compute_returns(rewards, gamma)\n",
        "\n",
        "    # update the policy network\n",
        "    reinforce_update(policy_net, states, actions, returns)\n",
        "\n",
        "    # evaluate the policy every 25 episodes\n",
        "    if episode % 25 == 0:\n",
        "        total = 0\n",
        "        # run 10 evalution episode to get average reward\n",
        "        for _ in range(10):\n",
        "            s,_ = env.reset()\n",
        "            ep_reward = 0\n",
        "            while True:\n",
        "                # select the action with highest probability\n",
        "                s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
        "                probs = policy_net(s_tensor)[0].numpy()\n",
        "                a = np.argmax(probs)\n",
        "\n",
        "                # take the action in the environment\n",
        "                s, r, terminated, truncated,_ = env.step(a)\n",
        "                # accumulate reward\n",
        "                ep_reward += r\n",
        "                if terminated or truncated:\n",
        "                    break\n",
        "            # accumulate total reward over 10 episodes\n",
        "            total += ep_reward\n",
        "\n",
        "        # compute average reward\n",
        "        avg = total / 10\n",
        "        print(f\"Episode {episode}, Avg Reward: {avg}\")\n",
        "\n",
        "        if avg >= 500:\n",
        "            print(\"==== Solved! ====\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO-XIp_XUglc"
      },
      "source": [
        "### Exercise 2 - Deep Deterministic Policy Gradient (DDPG)\n",
        "\n",
        "**Summary:** Implement the DDPG algorithm and use it to solve the ```Pusher-v4``` environment. If the   \n",
        "physics do not work as supposed , you might have to explicitly install mujoco version 2.3.0.\n",
        "\n",
        "\n",
        "**Provided Code:** Feel free to re-use code from previous exercises. Below I have provided you with   \n",
        "an implementation for soft weight-updates using keras.\n",
        "\n",
        "\n",
        "**Your Tasks in this exercise:**\n",
        "1. Implement DDPG\n",
        "2. Solve the ```Pusher-v4``` environment.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "piaYjp6bUglc"
      },
      "outputs": [],
      "source": [
        "def update_target_weights(source, target, tau=0.99):\n",
        "    ''' Performs a soft update as:\n",
        "        target <- tau * tar + (1-tau) * src\n",
        "        This is the other way as in our previous implementation following the DDPG paper.\n",
        "    '''\n",
        "    for i in range(len(source.layers)):\n",
        "\n",
        "        layer_weights_list_source = source.layers[i].get_weights()\n",
        "        layer_weights_list_target = target.layers[i].get_weights()\n",
        "\n",
        "        new_weights = []\n",
        "        for (w_src, w_target) in zip(layer_weights_list_source, layer_weights_list_target):\n",
        "            w_target = w_target* tau + (1.0-tau) * w_src\n",
        "            new_weights.append(w_target)\n",
        "\n",
        "        target.layers[i].set_weights(new_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8WtYny1IUgld"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# create actor - input is state_dim, output is action_dim\n",
        "def build_actor(state_dim, action_dim, action_high):\n",
        "    inputs = layers.Input(shape=(state_dim,))\n",
        "    x = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(action_dim, activation=\"tanh\")(x)\n",
        "    outputs = outputs * action_high  # we can scale this to the action range\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# create ciritc - input is state_dim + action_dim, output is Q-value\n",
        "def build_critic(state_dim, action_dim):\n",
        "    s_in = layers.Input(shape=(state_dim,))\n",
        "    a_in = layers.Input(shape=(action_dim,))\n",
        "    \n",
        "    x = layers.Concatenate()([s_in, a_in]) # concat state and action\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    q = layers.Dense(1)(x) # output Q-Value\n",
        "\n",
        "    return tf.keras.Model([s_in, a_in], q)\n",
        "\n",
        "# replay buffer to store\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, state_dim, action_dim):\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "        \n",
        "        # create the storage arrays\n",
        "        self.s = np.zeros((max_size, state_dim))\n",
        "        self.a = np.zeros((max_size, action_dim))\n",
        "        self.r = np.zeros((max_size, 1))\n",
        "        self.s2 = np.zeros((max_size, state_dim))\n",
        "        self.d = np.zeros((max_size, 1))\n",
        "\n",
        "    def store(self, s, a, r, s2, d):\n",
        "        # store the transition at the current pointer\n",
        "        self.s[self.ptr] = s\n",
        "        self.a[self.ptr] = a\n",
        "        self.r[self.ptr] = r\n",
        "        self.s2[self.ptr] = s2\n",
        "        self.d[self.ptr] = d\n",
        "        \n",
        "        # move the pointer (ring buffer)\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # random indices and return them\n",
        "        idx = np.random.choice(self.size, batch_size)\n",
        "        return (\n",
        "            self.s[idx],\n",
        "            self.a[idx],\n",
        "            self.r[idx],\n",
        "            self.s2[idx],\n",
        "            self.d[idx],\n",
        "        )\n",
        "\n",
        "@tf.function\n",
        "def train_step(actor, critic,\n",
        "               actor_target, critic_target,\n",
        "               actor_optimizer, critic_optimizer,\n",
        "               batch, gamma):\n",
        "\n",
        "    # unpack the batch\n",
        "    s, a, r, s2, d = batch\n",
        "\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        a2 = actor_target(s2) # next action from target actor\n",
        "        q_target = critic_target([s2, a2]) # target Q-value\n",
        "        y = r + gamma * (1.0 - d) * q_target # compute target\n",
        "        q = critic([s, a]) # current Q-value\n",
        "        critic_loss = tf.reduce_mean((q - y)**2) # compute critic loss\n",
        "\n",
        "    critic_grads = tape.gradient(critic_loss, critic.trainable_variables) # compute gradients\n",
        "    critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables)) # apply gradients\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        actions = actor(s) # actions from current actor\n",
        "        actor_loss = -tf.reduce_mean(critic([s, actions])) # compute actor loss\n",
        "\n",
        "    actor_grads = tape.gradient(actor_loss, actor.trainable_variables) # compute actor gradients\n",
        "    actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables)) # apply actor gradients\n",
        "\n",
        "# evaluate the policy over a number of episodes\n",
        "def evaluate_policy(env, actor, episodes=10):\n",
        "    total_reward = 0.0\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            s_tensor = tf.convert_to_tensor(s[None], dtype=tf.float32)\n",
        "            a = actor(s_tensor)[0].numpy()\n",
        "            a = np.clip(a, env.action_space.low, env.action_space.high)\n",
        "\n",
        "            s, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            ep_reward += r\n",
        "\n",
        "        total_reward += ep_reward\n",
        "\n",
        "    return total_reward / episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0, Avg Eval Reward: -83.42\n",
            "Episode 25, Avg Eval Reward: -69.65\n",
            "Episode 50, Avg Eval Reward: -84.22\n",
            "Episode 75, Avg Eval Reward: -205.12\n",
            "Episode 100, Avg Eval Reward: -168.78\n",
            "Episode 125, Avg Eval Reward: -245.63\n",
            "Episode 150, Avg Eval Reward: -185.96\n",
            "Episode 175, Avg Eval Reward: -242.61\n",
            "Episode 200, Avg Eval Reward: -197.02\n",
            "Episode 225, Avg Eval Reward: -205.68\n",
            "Episode 250, Avg Eval Reward: -150.68\n",
            "Episode 275, Avg Eval Reward: -216.60\n",
            "Episode 300, Avg Eval Reward: -204.50\n",
            "Episode 325, Avg Eval Reward: -161.54\n",
            "Episode 350, Avg Eval Reward: -197.59\n",
            "Episode 375, Avg Eval Reward: -199.32\n",
            "Episode 400, Avg Eval Reward: -186.98\n",
            "Episode 425, Avg Eval Reward: -172.55\n",
            "Episode 450, Avg Eval Reward: -218.54\n",
            "Episode 475, Avg Eval Reward: -176.08\n",
            "Episode 500, Avg Eval Reward: -172.33\n",
            "Episode 525, Avg Eval Reward: -192.98\n",
            "Episode 550, Avg Eval Reward: -186.56\n",
            "Episode 575, Avg Eval Reward: -164.45\n",
            "Episode 600, Avg Eval Reward: -171.14\n",
            "Episode 625, Avg Eval Reward: -179.38\n",
            "Episode 650, Avg Eval Reward: -167.37\n",
            "Episode 675, Avg Eval Reward: -164.13\n",
            "Episode 700, Avg Eval Reward: -137.93\n",
            "Episode 725, Avg Eval Reward: -155.21\n",
            "Episode 750, Avg Eval Reward: -138.26\n",
            "Episode 775, Avg Eval Reward: -155.43\n",
            "Episode 800, Avg Eval Reward: -189.30\n",
            "Episode 825, Avg Eval Reward: -172.32\n",
            "Episode 850, Avg Eval Reward: -186.68\n",
            "Episode 875, Avg Eval Reward: -181.92\n",
            "Episode 900, Avg Eval Reward: -194.11\n",
            "Episode 925, Avg Eval Reward: -203.06\n",
            "Episode 950, Avg Eval Reward: -179.35\n",
            "Episode 975, Avg Eval Reward: -155.82\n",
            "Episode 1000, Avg Eval Reward: -172.64\n",
            "Episode 1025, Avg Eval Reward: -165.78\n",
            "Episode 1050, Avg Eval Reward: -169.08\n",
            "Episode 1075, Avg Eval Reward: -194.57\n",
            "Episode 1100, Avg Eval Reward: -176.11\n",
            "Episode 1125, Avg Eval Reward: -162.29\n",
            "Episode 1150, Avg Eval Reward: -177.06\n",
            "Episode 1175, Avg Eval Reward: -151.83\n",
            "Episode 1200, Avg Eval Reward: -151.92\n",
            "Episode 1225, Avg Eval Reward: -168.61\n",
            "Episode 1250, Avg Eval Reward: -206.07\n",
            "Episode 1275, Avg Eval Reward: -186.04\n",
            "Episode 1300, Avg Eval Reward: -188.36\n",
            "Episode 1325, Avg Eval Reward: -147.55\n",
            "Episode 1350, Avg Eval Reward: -187.82\n",
            "Episode 1375, Avg Eval Reward: -184.91\n",
            "Episode 1400, Avg Eval Reward: -173.11\n",
            "Episode 1425, Avg Eval Reward: -167.46\n",
            "Episode 1450, Avg Eval Reward: -122.89\n",
            "Episode 1475, Avg Eval Reward: -131.11\n",
            "Episode 1500, Avg Eval Reward: -154.75\n",
            "Episode 1525, Avg Eval Reward: -130.91\n",
            "Episode 1550, Avg Eval Reward: -160.32\n",
            "Episode 1575, Avg Eval Reward: -181.24\n",
            "Episode 1600, Avg Eval Reward: -172.12\n",
            "Episode 1625, Avg Eval Reward: -156.11\n",
            "Episode 1650, Avg Eval Reward: -142.16\n",
            "Episode 1675, Avg Eval Reward: -197.57\n",
            "Episode 1700, Avg Eval Reward: -189.41\n",
            "Episode 1725, Avg Eval Reward: -190.30\n",
            "Episode 1750, Avg Eval Reward: -137.06\n",
            "Episode 1775, Avg Eval Reward: -166.71\n",
            "Episode 1800, Avg Eval Reward: -175.27\n",
            "Episode 1825, Avg Eval Reward: -183.27\n",
            "Episode 1850, Avg Eval Reward: -186.03\n",
            "Episode 1875, Avg Eval Reward: -177.73\n",
            "Episode 1900, Avg Eval Reward: -142.30\n",
            "Episode 1925, Avg Eval Reward: -134.97\n",
            "Episode 1950, Avg Eval Reward: -160.39\n",
            "Episode 1975, Avg Eval Reward: -143.73\n",
            "Episode 2000, Avg Eval Reward: -130.47\n",
            "Episode 2025, Avg Eval Reward: -167.03\n",
            "Episode 2050, Avg Eval Reward: -121.38\n",
            "Episode 2075, Avg Eval Reward: -130.07\n",
            "Episode 2100, Avg Eval Reward: -149.02\n",
            "Episode 2125, Avg Eval Reward: -147.00\n",
            "Episode 2150, Avg Eval Reward: -148.57\n",
            "Episode 2175, Avg Eval Reward: -154.16\n",
            "Episode 2200, Avg Eval Reward: -162.23\n",
            "Episode 2225, Avg Eval Reward: -148.01\n",
            "Episode 2250, Avg Eval Reward: -123.35\n",
            "Episode 2275, Avg Eval Reward: -135.63\n",
            "Episode 2300, Avg Eval Reward: -132.20\n",
            "Episode 2325, Avg Eval Reward: -124.47\n",
            "Episode 2350, Avg Eval Reward: -134.22\n",
            "Episode 2375, Avg Eval Reward: -130.64\n",
            "Episode 2400, Avg Eval Reward: -119.67\n",
            "Episode 2425, Avg Eval Reward: -120.38\n",
            "Episode 2450, Avg Eval Reward: -129.90\n",
            "Episode 2475, Avg Eval Reward: -140.49\n",
            "Episode 2500, Avg Eval Reward: -142.52\n",
            "Episode 2525, Avg Eval Reward: -133.30\n",
            "Episode 2550, Avg Eval Reward: -136.19\n",
            "Episode 2575, Avg Eval Reward: -143.82\n",
            "Episode 2600, Avg Eval Reward: -154.54\n",
            "Episode 2625, Avg Eval Reward: -165.99\n",
            "Episode 2650, Avg Eval Reward: -142.42\n",
            "Episode 2675, Avg Eval Reward: -145.19\n",
            "Episode 2700, Avg Eval Reward: -153.32\n",
            "Episode 2725, Avg Eval Reward: -175.48\n",
            "Episode 2750, Avg Eval Reward: -175.63\n",
            "Episode 2775, Avg Eval Reward: -176.64\n",
            "Episode 2800, Avg Eval Reward: -167.99\n",
            "Episode 2825, Avg Eval Reward: -168.39\n",
            "Episode 2850, Avg Eval Reward: -176.75\n",
            "Episode 2875, Avg Eval Reward: -178.18\n",
            "Episode 2900, Avg Eval Reward: -160.43\n",
            "Episode 2925, Avg Eval Reward: -160.65\n",
            "Episode 2950, Avg Eval Reward: -164.00\n",
            "Episode 2975, Avg Eval Reward: -149.44\n",
            "Episode 3000, Avg Eval Reward: -150.63\n",
            "Episode 3025, Avg Eval Reward: -158.37\n",
            "Episode 3050, Avg Eval Reward: -166.57\n",
            "Episode 3075, Avg Eval Reward: -158.97\n",
            "Episode 3100, Avg Eval Reward: -145.00\n",
            "Episode 3125, Avg Eval Reward: -172.59\n",
            "Episode 3150, Avg Eval Reward: -148.93\n",
            "Episode 3175, Avg Eval Reward: -157.87\n",
            "Episode 3200, Avg Eval Reward: -176.10\n",
            "Episode 3225, Avg Eval Reward: -180.16\n",
            "Episode 3250, Avg Eval Reward: -170.72\n",
            "Episode 3275, Avg Eval Reward: -172.66\n",
            "Episode 3300, Avg Eval Reward: -171.25\n",
            "Episode 3325, Avg Eval Reward: -169.53\n",
            "Episode 3350, Avg Eval Reward: -173.29\n",
            "Episode 3375, Avg Eval Reward: -157.17\n",
            "Episode 3400, Avg Eval Reward: -170.87\n",
            "Episode 3425, Avg Eval Reward: -181.02\n",
            "Episode 3450, Avg Eval Reward: -174.15\n",
            "Episode 3475, Avg Eval Reward: -176.91\n",
            "Episode 3500, Avg Eval Reward: -175.75\n",
            "Episode 3525, Avg Eval Reward: -165.06\n",
            "Episode 3550, Avg Eval Reward: -191.18\n",
            "Episode 3575, Avg Eval Reward: -146.02\n",
            "Episode 3600, Avg Eval Reward: -184.16\n",
            "Episode 3625, Avg Eval Reward: -159.39\n",
            "Episode 3650, Avg Eval Reward: -157.73\n",
            "Episode 3675, Avg Eval Reward: -167.41\n",
            "Episode 3700, Avg Eval Reward: -175.71\n",
            "Episode 3725, Avg Eval Reward: -159.72\n",
            "Episode 3750, Avg Eval Reward: -161.50\n",
            "Episode 3775, Avg Eval Reward: -173.44\n",
            "Episode 3800, Avg Eval Reward: -180.21\n",
            "Episode 3825, Avg Eval Reward: -149.54\n",
            "Episode 3850, Avg Eval Reward: -181.37\n",
            "Episode 3875, Avg Eval Reward: -181.04\n",
            "Episode 3900, Avg Eval Reward: -164.49\n",
            "Episode 3925, Avg Eval Reward: -151.31\n",
            "Episode 3950, Avg Eval Reward: -161.76\n",
            "Episode 3975, Avg Eval Reward: -134.75\n",
            "Episode 4000, Avg Eval Reward: -156.55\n",
            "Episode 4025, Avg Eval Reward: -156.60\n",
            "Episode 4050, Avg Eval Reward: -156.00\n",
            "Episode 4075, Avg Eval Reward: -166.15\n",
            "Episode 4100, Avg Eval Reward: -205.51\n",
            "Episode 4125, Avg Eval Reward: -181.20\n",
            "Episode 4150, Avg Eval Reward: -165.77\n",
            "Episode 4175, Avg Eval Reward: -160.05\n",
            "Episode 4200, Avg Eval Reward: -188.84\n",
            "Episode 4225, Avg Eval Reward: -189.59\n",
            "Episode 4250, Avg Eval Reward: -182.45\n",
            "Episode 4275, Avg Eval Reward: -169.71\n",
            "Episode 4300, Avg Eval Reward: -173.34\n",
            "Episode 4325, Avg Eval Reward: -182.13\n",
            "Episode 4350, Avg Eval Reward: -191.15\n",
            "Episode 4375, Avg Eval Reward: -184.33\n",
            "Episode 4400, Avg Eval Reward: -146.47\n",
            "Episode 4425, Avg Eval Reward: -178.61\n",
            "Episode 4450, Avg Eval Reward: -162.63\n",
            "Episode 4475, Avg Eval Reward: -160.51\n",
            "Episode 4500, Avg Eval Reward: -168.92\n",
            "Episode 4525, Avg Eval Reward: -157.54\n",
            "Episode 4550, Avg Eval Reward: -148.42\n",
            "Episode 4575, Avg Eval Reward: -157.73\n",
            "Episode 4600, Avg Eval Reward: -149.07\n",
            "Episode 4625, Avg Eval Reward: -149.63\n",
            "Episode 4650, Avg Eval Reward: -137.40\n",
            "Episode 4675, Avg Eval Reward: -142.76\n",
            "Episode 4700, Avg Eval Reward: -172.31\n",
            "Episode 4725, Avg Eval Reward: -171.10\n",
            "Episode 4750, Avg Eval Reward: -154.35\n",
            "Episode 4775, Avg Eval Reward: -150.66\n",
            "Episode 4800, Avg Eval Reward: -139.83\n",
            "Episode 4825, Avg Eval Reward: -158.22\n",
            "Episode 4850, Avg Eval Reward: -171.15\n",
            "Episode 4875, Avg Eval Reward: -138.62\n",
            "Episode 4900, Avg Eval Reward: -126.81\n",
            "Episode 4925, Avg Eval Reward: -145.57\n",
            "Episode 4950, Avg Eval Reward: -165.20\n",
            "Episode 4975, Avg Eval Reward: -160.59\n",
            "Episode 5000, Avg Eval Reward: -158.84\n",
            "Episode 5025, Avg Eval Reward: -157.52\n",
            "Episode 5050, Avg Eval Reward: -118.77\n",
            "Episode 5075, Avg Eval Reward: -167.27\n",
            "Episode 5100, Avg Eval Reward: -160.91\n",
            "Episode 5125, Avg Eval Reward: -168.40\n",
            "Episode 5150, Avg Eval Reward: -161.08\n",
            "Episode 5175, Avg Eval Reward: -134.78\n",
            "Episode 5200, Avg Eval Reward: -147.72\n",
            "Episode 5225, Avg Eval Reward: -157.31\n",
            "Episode 5250, Avg Eval Reward: -175.23\n",
            "Episode 5275, Avg Eval Reward: -174.43\n",
            "Episode 5300, Avg Eval Reward: -168.38\n",
            "Episode 5325, Avg Eval Reward: -173.18\n",
            "Episode 5350, Avg Eval Reward: -155.62\n",
            "Episode 5375, Avg Eval Reward: -153.68\n",
            "Episode 5400, Avg Eval Reward: -173.13\n",
            "Episode 5425, Avg Eval Reward: -167.50\n",
            "Episode 5450, Avg Eval Reward: -184.25\n",
            "Episode 5475, Avg Eval Reward: -176.67\n",
            "Episode 5500, Avg Eval Reward: -185.17\n",
            "Episode 5525, Avg Eval Reward: -169.51\n",
            "Episode 5550, Avg Eval Reward: -170.96\n",
            "Episode 5575, Avg Eval Reward: -161.26\n",
            "Episode 5600, Avg Eval Reward: -160.43\n",
            "Episode 5625, Avg Eval Reward: -171.48\n",
            "Episode 5650, Avg Eval Reward: -175.34\n",
            "Episode 5675, Avg Eval Reward: -162.88\n",
            "Episode 5700, Avg Eval Reward: -167.81\n",
            "Episode 5725, Avg Eval Reward: -176.03\n",
            "Episode 5750, Avg Eval Reward: -164.58\n",
            "Episode 5775, Avg Eval Reward: -168.00\n",
            "Episode 5800, Avg Eval Reward: -156.61\n",
            "Episode 5825, Avg Eval Reward: -165.60\n",
            "Episode 5850, Avg Eval Reward: -177.98\n",
            "Episode 5875, Avg Eval Reward: -165.29\n",
            "Episode 5900, Avg Eval Reward: -165.64\n",
            "Episode 5925, Avg Eval Reward: -164.51\n",
            "Episode 5950, Avg Eval Reward: -171.03\n",
            "Episode 5975, Avg Eval Reward: -171.13\n",
            "Episode 6000, Avg Eval Reward: -179.03\n",
            "Episode 6025, Avg Eval Reward: -172.37\n",
            "Episode 6050, Avg Eval Reward: -173.66\n",
            "Episode 6075, Avg Eval Reward: -182.39\n",
            "Episode 6100, Avg Eval Reward: -148.64\n",
            "Episode 6125, Avg Eval Reward: -177.81\n",
            "Episode 6150, Avg Eval Reward: -182.83\n",
            "Episode 6175, Avg Eval Reward: -163.82\n",
            "Episode 6200, Avg Eval Reward: -164.64\n",
            "Episode 6225, Avg Eval Reward: -169.81\n",
            "Episode 6250, Avg Eval Reward: -181.03\n",
            "Episode 6275, Avg Eval Reward: -169.31\n",
            "Episode 6300, Avg Eval Reward: -172.25\n",
            "Episode 6325, Avg Eval Reward: -160.84\n",
            "Episode 6350, Avg Eval Reward: -179.99\n",
            "Episode 6375, Avg Eval Reward: -185.97\n",
            "Episode 6400, Avg Eval Reward: -169.44\n",
            "Episode 6425, Avg Eval Reward: -175.76\n",
            "Episode 6450, Avg Eval Reward: -176.51\n",
            "Episode 6475, Avg Eval Reward: -168.56\n",
            "Episode 6500, Avg Eval Reward: -161.89\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[32m/tmp/ipykernel_902140/1300865272.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     63\u001b[39m             )\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m             \u001b[38;5;66;03m# soft update target networks\u001b[39;00m\n\u001b[32m     66\u001b[39m             update_target_weights(actor, actor_target)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m             update_target_weights(critic, critic_target)\n\u001b[32m     68\u001b[39m \n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# evaluate the policy every 25 episodes\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m episode % \u001b[32m25\u001b[39m == \u001b[32m0\u001b[39m:\n",
            "\u001b[32m/tmp/ipykernel_902140/408878394.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(source, target, tau)\u001b[39m\n\u001b[32m      4\u001b[39m         This \u001b[38;5;28;01mis\u001b[39;00m the other way \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m our previous implementation following the DDPG paper.\n\u001b[32m      5\u001b[39m     '''\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m range(len(source.layers)):\n\u001b[32m      7\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         layer_weights_list_source = source.layers[i].get_weights()\n\u001b[32m      9\u001b[39m         layer_weights_list_target = target.layers[i].get_weights()\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m         new_weights = []\n",
            "\u001b[32m~/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/keras/src/layers/layer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    726\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m get_weights(self):\n\u001b[32m    727\u001b[39m         \u001b[33m\"\"\"Return the values of `layer.weights` as a list of NumPy arrays.\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [v.numpy() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m self.weights]\n",
            "\u001b[32m~/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m numpy(self):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.value.numpy()\n",
            "\u001b[32m~/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    711\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m numpy(self):\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m self.read_value().numpy()\n\u001b[32m    714\u001b[39m     raise NotImplementedError(\n\u001b[32m    715\u001b[39m         \u001b[33m\"numpy() is only available when eager execution is enabled.\"\u001b[39m)\n",
            "\u001b[32m~/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(\u001b[33m\"Read\"\u001b[39m):\n\u001b[32m    881\u001b[39m       value = self._read_variable_op()\n\u001b[32m    882\u001b[39m     \u001b[38;5;66;03m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[32m    883\u001b[39m     \u001b[38;5;66;03m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m array_ops.identity(value)\n",
            "\u001b[32m~/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m     89\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m     90\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m     91\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
            "\u001b[32m~/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "\u001b[32m~/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1261\u001b[39m \n\u001b[32m   1262\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1263\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1266\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1267\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1268\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
            "\u001b[32m~/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/tensorflow/python/ops/array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input, name)\u001b[39m\n\u001b[32m    306\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly() \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m hasattr(input, \u001b[33m\"graph\"\u001b[39m):\n\u001b[32m    307\u001b[39m     \u001b[38;5;66;03m# Make sure we get an input with handle data attached from the resource\u001b[39;00m\n\u001b[32m    308\u001b[39m     \u001b[38;5;66;03m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[32m    309\u001b[39m     input = ops.convert_to_tensor(input)\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m   ret = gen_array_ops.identity(input, name=name)\n\u001b[32m    311\u001b[39m   \u001b[38;5;66;03m# Propagate handles data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[32m    312\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m hasattr(input, \u001b[33m\"_handle_data\"\u001b[39m):\n\u001b[32m    313\u001b[39m     ret._handle_data = input._handle_data  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
            "\u001b[32m~/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input, name)\u001b[39m\n\u001b[32m   4227\u001b[39m         _ctx, \u001b[33m\"Identity\"\u001b[39m, name, input)\n\u001b[32m   4228\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   4229\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   4230\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m4231\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   4232\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   4233\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   4234\u001b[39m       return identity_eager_fallback(\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"Pusher-v4\")\n",
        "\n",
        "# get dimensions from env\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "action_high = env.action_space.high[0]\n",
        "\n",
        "# create networks\n",
        "actor = build_actor(state_dim, action_dim, action_high)\n",
        "critic = build_critic(state_dim, action_dim)\n",
        "\n",
        "# create target networks\n",
        "actor_target = build_actor(state_dim, action_dim, action_high)\n",
        "critic_target = build_critic(state_dim, action_dim)\n",
        "\n",
        "# init the weights\n",
        "actor_target.set_weights(actor.get_weights())\n",
        "critic_target.set_weights(critic.get_weights())\n",
        "\n",
        "actor_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "critic_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "\n",
        "# create replay buffer\n",
        "buffer = ReplayBuffer(1_000_000, state_dim, action_dim)\n",
        "\n",
        "# hyperparameters\n",
        "gamma = 0.99\n",
        "batch_size = 1024\n",
        "\n",
        "\n",
        "for episode in range(100000):\n",
        "    noise_std = max(0.1, 0.3 * (1 - episode / 50_000))\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # action with noise for exploration\n",
        "        a = actor(s[None]).numpy()[0]\n",
        "        a += np.random.normal(0, noise_std, size=action_dim)\n",
        "        a = np.clip(a, env.action_space.low, env.action_space.high) # clip action to valid range\n",
        "\n",
        "        # take action in env\n",
        "        s2, r, terminated, truncated, _ = env.step(a)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # store transition in replay buffer\n",
        "        buffer.store(s, a, r, s2, float(done))\n",
        "        s = s2 # move to next state\n",
        "\n",
        "        # train if we have enough samples for a batch\n",
        "        if buffer.size >= batch_size:\n",
        "            batch = buffer.sample(batch_size) # sample a batch\n",
        "            batch = [tf.convert_to_tensor(x, dtype=tf.float32) for x in batch] # convert to tensors\n",
        "\n",
        "            # train step\n",
        "            train_step(\n",
        "                actor, critic,\n",
        "                actor_target, critic_target,\n",
        "                actor_optimizer, critic_optimizer,\n",
        "                batch, gamma\n",
        "            )\n",
        "\n",
        "            # soft update target networks\n",
        "            update_target_weights(actor, actor_target)\n",
        "            update_target_weights(critic, critic_target)\n",
        "\n",
        "    # evaluate the policy every 25 episodes\n",
        "    if episode % 25 == 0:\n",
        "        avg_reward = evaluate_policy(env, actor, episodes=10)\n",
        "        print(f\"Episode {episode}, Avg Eval Reward: {avg_reward:.2f}\")\n",
        "\n",
        "        # best theoretical possible is 0. try to get close to that. Chatgpt says bigger than -25 is good.\n",
        "        if avg_reward > -25:\n",
        "            print(\"==== Policy is performing well! ====\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH8vzcNAUgld"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nico/projects/ReinforcementLearning/rl/lib/python3.12/site-packages/matplotlib/animation.py:908: UserWarning: Animation was deleted without rendering anything. This is most likely not intended. To prevent deletion, assign the Animation to a variable, e.g. `anim`, that exists until you output the Animation using `plt.show()` or `anim.save()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.animation.FuncAnimation at 0x792cac064b60>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make(\"Pusher-v4\", render_mode=\"rgb_array\")\n",
        "s_t, _ = env.reset()\n",
        "\n",
        "done = False\n",
        "frames = []\n",
        "\n",
        "while not done:\n",
        "    # Select action from policy (actor outputs continuous action)\n",
        "    # ensure observation array dtype matches the model expectation\n",
        "    a_t = actor_target(np.array([s_t], dtype=np.float32))[0].numpy().squeeze()\n",
        "    \n",
        "    s_tplus1, r_tplus1, terminated, truncated, info = env.step(a_t)\n",
        "    done = terminated or truncated\n",
        "    \n",
        "    # Get the rendered frame\n",
        "    frame = env.render()\n",
        "    frames.append(frame)\n",
        "    \n",
        "    s_t = s_tplus1\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Display frames inline as an animation\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "fig = plt.figure()\n",
        "patch = plt.imshow(frames[0])\n",
        "\n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "\n",
        "ani = animation.FuncAnimation(fig, animate, frames=len(frames), interval=30)\n",
        "plt.close()\n",
        "display.display(ani)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import imageio\n",
        "\n",
        "imageio.mimsave('pusher.gif', frames, fps=30)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
